{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of writing_a_training_loop_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dupeljan/collab/blob/main/Copy_of_writing_a_training_loop_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b518b04cbfe0"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daf323e33b84"
      },
      "source": [
        "# Writing a training loop from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae2407ad926f"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W10xYtOImrI6"
      },
      "source": [
        "Setup dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suevcQFEmo1g"
      },
      "source": [
        "# Prepare the training dataset.\r\n",
        "batch_size = 64\r\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n",
        "x_train = np.reshape(x_train, (-1, 784))\r\n",
        "x_test = np.reshape(x_test, (-1, 784))\r\n",
        "\r\n",
        "# Reserve 10,000 samples for validation.\r\n",
        "x_val = x_train[-10000:]\r\n",
        "y_val = y_train[-10000:]\r\n",
        "x_train = x_train[:-10000]\r\n",
        "y_train = y_train[:-10000]\r\n",
        "\r\n",
        "# Prepare the training dataset.\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\r\n",
        "\r\n",
        "# Prepare the validation dataset.\r\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\r\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk9d-GNcosMi"
      },
      "source": [
        "# Eager mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaa775ce7dab"
      },
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2c6257b8d02"
      },
      "source": [
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6374be9e3d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d944be3d-49d5-48db-baeb-589b0a9de50f"
      },
      "source": [
        "def train(model, epochs = 2):\n",
        "  for epoch in range(epochs):\n",
        "      print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "      # Iterate over the batches of the dataset.\n",
        "      for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "          # Open a GradientTape to record the operations run\n",
        "          # during the forward pass, which enables auto-differentiation.\n",
        "          with tf.GradientTape() as tape:\n",
        "\n",
        "              # Run the forward pass of the layer.\n",
        "              # The operations that the layer applies\n",
        "              # to its inputs are going to be recorded\n",
        "              # on the GradientTape.\n",
        "              logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "              # Compute the loss value for this minibatch.\n",
        "              loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "          # Use the gradient tape to automatically retrieve\n",
        "          # the gradients of the trainable variables with respect to the loss.\n",
        "          grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "          # Run one step of gradient descent by updating\n",
        "          # the value of the variables to minimize the loss.\n",
        "          optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "          # Log every 200 batches.\n",
        "          if step % 200 == 0:\n",
        "              print(\n",
        "                  \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                  % (step, float(loss_value))\n",
        "              )\n",
        "              print(\"Seen so far: %s samples\" % ((step + 1) * 64))\n",
        "train(model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 84.5313\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 2.1334\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 0.7563\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 0.6857\n",
            "Seen so far: 38464 samples\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.6501\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 0.5389\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 0.5519\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 0.5096\n",
            "Seen so far: 38464 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPf7R0cTnGcf"
      },
      "source": [
        "Check trainable weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA74VQxKtLO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ef4eb7-f3f4-49e5-df7a-b868453dc892"
      },
      "source": [
        "[ x.name for x in model.trainable_weights]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dense_2/kernel:0',\n",
              " 'dense_2/bias:0',\n",
              " 'dense_3/kernel:0',\n",
              " 'dense_3/bias:0',\n",
              " 'predictions/kernel:0',\n",
              " 'predictions/bias:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukgwF6z90XqX"
      },
      "source": [
        "Freeze layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0W60mysyhYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f040df4-36ac-4652-dd0b-518baffdce14"
      },
      "source": [
        "def copy(weights):\r\n",
        "    res = []\r\n",
        "    for w in weights:\r\n",
        "      res.append(tf.identity(w))\r\n",
        "    return res \r\n",
        "\r\n",
        "\r\n",
        "weights = dict()\r\n",
        "# Freeze all layers besides the last one\r\n",
        "target_layer_name = 'predictions'\r\n",
        "for layer in model.layers:\r\n",
        "  if layer.weights:\r\n",
        "    if layer.name != target_layer_name:\r\n",
        "      layer.trainable = False\r\n",
        "    weights[layer.name] = copy(layer.weights)\r\n",
        "\r\n",
        "# Check trainable weights list\r\n",
        "[x.name for x in model.trainable_weights]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['predictions/kernel:0', 'predictions/bias:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFef2vRyyV5l"
      },
      "source": [
        "Train only last layer of model and check it\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYNWM3a10wlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c66247f-bd58-456a-a987-1199ff7498f0"
      },
      "source": [
        "train(model)\r\n",
        "\r\n",
        "def equal(w1, w2):\r\n",
        "    return [ bool(tf.math.reduce_all(tf.math.equal(w, w_saved))) \r\n",
        "              for w, w_saved in zip(w1, w2)]\r\n",
        "\r\n",
        "    \r\n",
        "for layer in model.layers:\r\n",
        "  if not layer.weights:\r\n",
        "    continue\r\n",
        "  if layer.name != target_layer_name:\r\n",
        "    assert all(equal(weights[layer.name], layer.weights))\r\n",
        "  else:\r\n",
        "    assert not all(equal(weights[layer.name], layer.weights))\r\n",
        "  print(\"{0} sqr diff: {1}\".format(\r\n",
        "        layer.name,\r\n",
        "        tf.math.reduce_sum(tf.math.pow(\r\n",
        "                        layer.weights[0] -  weights[layer.name][0], 2))\r\n",
        "    ))\r\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 0.3266\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 0.3251\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 0.6280\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 0.2726\n",
            "Seen so far: 38464 samples\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.2719\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 0.7506\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 0.3221\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 0.4017\n",
            "Seen so far: 38464 samples\n",
            "dense_2 sqr diff: 0.0\n",
            "dense_3 sqr diff: 0.0\n",
            "predictions sqr diff: 0.02466883882880211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfNvxi79m113"
      },
      "source": [
        "Freezing in eager mode is working!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGuDJooK8-bU"
      },
      "source": [
        "You can freeze layer, but you can't set weight trainable attribute. If layer have trainable == false this it just not lised at `model.trainable_weights`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2k3dfDr4yg3",
        "outputId": "d2f0a492-fc75-4042-d958-2bc71f48408b"
      },
      "source": [
        "for layer in model.layers:\r\n",
        "  if layer.name != target_layer_name:\r\n",
        "    for weight in layer.weights:\r\n",
        "      print('w', weight.name, weight.trainable)\r\n",
        "    print('l', layer.name, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l digits False\n",
            "w dense/kernel:0 True\n",
            "w dense/bias:0 True\n",
            "l dense False\n",
            "w dense_1/kernel:0 True\n",
            "w dense_1/bias:0 True\n",
            "l dense_1 False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX49Y4BbfNwi"
      },
      "source": [
        "# Non eager mode with GradTape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdEMe8EWjR_c"
      },
      "source": [
        "Setup train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgLjY_Swfz_D"
      },
      "source": [
        "@tf.function\n",
        "def train():\n",
        "  epochs = 3\n",
        "  for epoch in range(epochs):\n",
        "      print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "      # Iterate over the batches of the dataset.\n",
        "      for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "          # Open a GradientTape to record the operations run\n",
        "          # during the forward pass, which enables auto-differentiation.\n",
        "          with tf.GradientTape() as tape:\n",
        "\n",
        "              # Run the forward pass of the layer.\n",
        "              # The operations that the layer applies\n",
        "              # to its inputs are going to be recorded\n",
        "              # on the GradientTape.\n",
        "              logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "              # Compute the loss value for this minibatch.\n",
        "              loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "          # Use the gradient tape to automatically retrieve\n",
        "          # the gradients of the trainable variables with respect to the loss.\n",
        "          grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "          # Run one step of gradient descent by updating\n",
        "          # the value of the variables to minimize the loss.\n",
        "          optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr8ol9rkjc8U"
      },
      "source": [
        "Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCyP2bGJfz-2"
      },
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOFz3u8pjJDN"
      },
      "source": [
        "Setup trainable params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eR7G8SRfz_E",
        "outputId": "01a3347e-5aab-4ca6-e595-f167cbc78245"
      },
      "source": [
        "def copy(weights):\r\n",
        "    res = []\r\n",
        "    for w in weights:\r\n",
        "      res.append(tf.identity(w))\r\n",
        "    return res \r\n",
        "\r\n",
        "\r\n",
        "weights = dict()\r\n",
        "def save_weight():\r\n",
        "  global weights\r\n",
        "  for layer in model.layers:\r\n",
        "    if layer.weights:\r\n",
        "      weights[layer.name] = copy(layer.weights)\r\n",
        "\r\n",
        "save_weight()\r\n",
        "\r\n",
        "# Freeze all layers besides the last one\r\n",
        "target_layer_name = 'predictions'\r\n",
        "for layer in model.layers:\r\n",
        "  if layer.weights:\r\n",
        "    if layer.name != target_layer_name:\r\n",
        "      layer.trainable = False\r\n",
        "\r\n",
        "# Check trainable weights list\r\n",
        "[x.name for x in model.trainable_weights]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['predictions/kernel:0', 'predictions/bias:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLJ0ZCdPnfyY"
      },
      "source": [
        "Try to train only last layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmi_a3JYiexj",
        "outputId": "3c511c39-bf13-402f-994d-cb4bc4df288d"
      },
      "source": [
        "train()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "\n",
            "Start of epoch 1\n",
            "\n",
            "Start of epoch 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS5IrUxbnnqZ"
      },
      "source": [
        "Check freezing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_3EM4Xjfz_F",
        "outputId": "fe3a3df3-f422-4c67-a0a9-e645bebe2ed3"
      },
      "source": [
        "\r\n",
        "def equal(w1, w2):\r\n",
        "    return [ bool(tf.math.reduce_all(tf.math.equal(w, w_saved))) \r\n",
        "              for w, w_saved in zip(w1, w2)]\r\n",
        "\r\n",
        "def check_is_trainable():    \r\n",
        "  for layer in model.layers:\r\n",
        "    if not layer.weights:\r\n",
        "      continue\r\n",
        "    if layer.name != target_layer_name:\r\n",
        "      assert all(equal(weights[layer.name], layer.weights))\r\n",
        "    else:\r\n",
        "      assert not all(equal(weights[layer.name], layer.weights))\r\n",
        "    print(\"{0} sqr diff: {1}\".format(\r\n",
        "          layer.name,\r\n",
        "          tf.math.reduce_sum(tf.math.pow(\r\n",
        "                          layer.weights[0] -  weights[layer.name][0], 2))\r\n",
        "      ))\r\n",
        "\r\n",
        "check_is_trainable()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense_4 sqr diff: 0.0\n",
            "dense_5 sqr diff: 0.0\n",
            "predictions sqr diff: 13.99891471862793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGC5Dsk0nxuQ"
      },
      "source": [
        "That's ok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sGsJpPrkF31"
      },
      "source": [
        "Try to unfreeze all layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAj5e2_CkcPZ"
      },
      "source": [
        "for layer in model.layers:\r\n",
        "  layer.trainable = True"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZRs8reAklC-",
        "outputId": "1a378aad-2af5-4359-b6b6-0052d8a8af2a"
      },
      "source": [
        "save_weight()\r\n",
        "train()\r\n",
        "check_is_trainable()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense_4 sqr diff: 0.0\n",
            "dense_5 sqr diff: 0.0\n",
            "predictions sqr diff: 0.32455798983573914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vREZVdmn6HX"
      },
      "source": [
        "No success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kcAvfKv1ziY"
      },
      "source": [
        "# Keras API training mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Jm-7CNeu9M"
      },
      "source": [
        "Create model with custom train_step to be sure that trainable vars is got from self.trainable_variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3BKAOb3bP7O"
      },
      "source": [
        "class CustomModel(keras.Model):\r\n",
        "    def train_step(self, data):\r\n",
        "        # Unpack the data. Its structure depends on your model and\r\n",
        "        # on what you pass to `fit()`.\r\n",
        "        x, y = data\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            y_pred = self(x, training=True)  # Forward pass\r\n",
        "            # Compute the loss value\r\n",
        "            # (the loss function is configured in `compile()`)\r\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\r\n",
        "\r\n",
        "        # Compute gradients\r\n",
        "        trainable_vars = self.trainable_variables\r\n",
        "        print(\"Trainable vars\", trainable_vars)\r\n",
        "        gradients = tape.gradient(loss, trainable_vars)\r\n",
        "        # Update weights\r\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n",
        "        # Update metrics (includes the metric that tracks the loss)\r\n",
        "        self.compiled_metrics.update_state(y, y_pred)\r\n",
        "        # Return a dict mapping metric names to current value\r\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtVYOTff4ahW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9106a0b-600a-4eaa-c196-b23d2ebdc501"
      },
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"inputs\")\r\n",
        "x1 = layers.Dense(64, activation=\"relu\", name=\"dense1\")(inputs)\r\n",
        "x2 = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x1)\r\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x2)\r\n",
        "model = CustomModel(inputs=inputs, outputs=outputs)\r\n",
        "[x.trainable for x in model.layers]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True, True, True, True]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JMmqomgo5_d"
      },
      "source": [
        "Make callable which can freeze learning\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FDFVeHaqHmE"
      },
      "source": [
        "class FreezingCallback(tf.keras.callbacks.Callback):\r\n",
        "  @staticmethod \r\n",
        "  def copy(weights):\r\n",
        "    res = []\r\n",
        "    for w in weights:\r\n",
        "      res.append(tf.identity(w))\r\n",
        "    return res \r\n",
        "\r\n",
        "  @staticmethod\r\n",
        "  def have_weights(layer):\r\n",
        "    return hasattr(layer, 'weights') and layer.weights\r\n",
        "\r\n",
        "  @staticmethod\r\n",
        "  def equal(w1, w2):\r\n",
        "    return [(w.name, bool(tf.math.reduce_all(tf.math.equal(w, w_saved)))) \r\n",
        "              for w, w_saved in zip(w1, w2)]\r\n",
        "    \r\n",
        "  def __init__(self, layers_to_freeze, epoch_to_freeze):\r\n",
        "    super().__init__()\r\n",
        "    self.layers_to_freeze = layers_to_freeze\r\n",
        "    self.epoch_to_freeze = epoch_to_freeze\r\n",
        "    self.weights = dict()\r\n",
        "\r\n",
        "  def on_epoch_begin(self, epoch, logs=None):\r\n",
        "    # Keep weights before epoch\r\n",
        "    for layer in self.model.layers:\r\n",
        "      if self.have_weights(layer):\r\n",
        "        self.weights[layer.name] = self.copy(layer.weights)\r\n",
        "        assert all([x[1] for x in \r\n",
        "                   self.equal(layer.weights, self.weights[layer.name])])\r\n",
        "        \r\n",
        "    # Freeze if it's time to freeze\r\n",
        "    if epoch == self.epoch_to_freeze:\r\n",
        "      for layer in self.model.layers:\r\n",
        "        if layer.name in self.layers_to_freeze:\r\n",
        "          print(f\"Freeze layer {layer.name}\")\r\n",
        "          layer.trainable = False\r\n",
        "\r\n",
        "    print(\"Trainable status:\")\r\n",
        "    for layer in self.model.layers:\r\n",
        "      print(f\"Layer {layer.name} trainable = {layer.trainable}\")\r\n",
        "  \r\n",
        "  def on_epoch_end(self, epoch, logs=None):\r\n",
        "    print(logs)\r\n",
        "    for layer in self.model.layers:\r\n",
        "      if self.have_weights(layer):\r\n",
        "        equal = self.equal(layer.weights, self.weights[layer.name])\r\n",
        "        print(f\"Layer {layer.name} changed? \"\r\n",
        "              f\"{[(x[0], not x[1]) for x in equal]}\")\r\n",
        "        print(\"sqr diff {}\".format(\r\n",
        "            tf.math.reduce_sum(tf.math.pow(\r\n",
        "                        layer.weights[0] - self.weights[layer.name][0], 2))))\r\n",
        "        # Keep new weights\r\n",
        "        self.weights[layer.name] = self.copy(layer.weights)\r\n",
        "\r\n",
        "\r\n",
        "freezing_callback = FreezingCallback(layers_to_freeze=['dense1', 'dense2', ],#, 'predictions'],\r\n",
        "                                      epoch_to_freeze=4)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_SUGKadOwoh",
        "outputId": "ce9736dc-eddd-42c3-a42a-c548d1ae3146"
      },
      "source": [
        "# You can change trainable status only before compileing\r\n",
        "for layer in model.layers:\r\n",
        "  layer.trainable = True\r\n",
        "[x.trainable for x in model.layers]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True, True, True, True]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUX79sJ0qFzK"
      },
      "source": [
        "compile_kwargs = {\r\n",
        "    'optimizer': keras.optimizers.SGD(learning_rate=1e-3), \r\n",
        "    'loss': keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\r\n",
        "}\r\n",
        "\r\n",
        "model.compile(\r\n",
        "    **compile_kwargs\r\n",
        ")\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeDKvkzKyE0K",
        "outputId": "1f4db480-cd7a-4913-d8ef-de6eda8231a9"
      },
      "source": [
        "# Setup trainable param\r\n",
        "#for layer in model.layers:\r\n",
        "#  layer.trainable = False\r\n",
        "#model.trainable = False\r\n",
        "\r\n",
        "model.fit(\r\n",
        "    x_train,\r\n",
        "    y_train,\r\n",
        "    batch_size=32,\r\n",
        "    epochs=10,\r\n",
        "    callbacks=[freezing_callback]\r\n",
        ")\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = True\n",
            "Layer dense2 trainable = True\n",
            "Layer predictions trainable = True\n",
            "Trainable vars [<tf.Variable 'dense1/kernel:0' shape=(784, 64) dtype=float32>, <tf.Variable 'dense1/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'dense2/kernel:0' shape=(64, 64) dtype=float32>, <tf.Variable 'dense2/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'predictions/kernel:0' shape=(64, 10) dtype=float32>, <tf.Variable 'predictions/bias:0' shape=(10,) dtype=float32>]\n",
            "Trainable vars [<tf.Variable 'dense1/kernel:0' shape=(784, 64) dtype=float32>, <tf.Variable 'dense1/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'dense2/kernel:0' shape=(64, 64) dtype=float32>, <tf.Variable 'dense2/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'predictions/kernel:0' shape=(64, 10) dtype=float32>, <tf.Variable 'predictions/bias:0' shape=(10,) dtype=float32>]\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 4.1225 - sparse_categorical_accuracy: 0.6869\n",
            "{'loss': 1.450222134590149, 'sparse_categorical_accuracy': 0.7686600089073181}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 4.3450422286987305\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 1.0224521160125732\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.4683196544647217\n",
            "Epoch 2/10\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = True\n",
            "Layer dense2 trainable = True\n",
            "Layer predictions trainable = True\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.5045 - sparse_categorical_accuracy: 0.8624\n",
            "{'loss': 0.4788740277290344, 'sparse_categorical_accuracy': 0.8707399964332581}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 0.19245858490467072\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 0.049054622650146484\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.025048810988664627\n",
            "Epoch 3/10\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = True\n",
            "Layer dense2 trainable = True\n",
            "Layer predictions trainable = True\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3813 - sparse_categorical_accuracy: 0.8938\n",
            "{'loss': 0.3720289170742035, 'sparse_categorical_accuracy': 0.8961399793624878}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 0.10038459300994873\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 0.02355032041668892\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.01199013739824295\n",
            "Epoch 4/10\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = True\n",
            "Layer dense2 trainable = True\n",
            "Layer predictions trainable = True\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3260 - sparse_categorical_accuracy: 0.9075\n",
            "{'loss': 0.32295843958854675, 'sparse_categorical_accuracy': 0.9081400036811829}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 0.06472229957580566\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 0.01680314913392067\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.007462570443749428\n",
            "Epoch 5/10\n",
            "Freeze layer dense1\n",
            "Freeze layer dense2\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = False\n",
            "Layer dense2 trainable = False\n",
            "Layer predictions trainable = True\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2941 - sparse_categorical_accuracy: 0.9152\n",
            "{'loss': 0.2915627062320709, 'sparse_categorical_accuracy': 0.915340006351471}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 0.04702324792742729\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 0.01073005236685276\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.005185134708881378\n",
            "Epoch 6/10\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = False\n",
            "Layer dense2 trainable = False\n",
            "Layer predictions trainable = True\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2601 - sparse_categorical_accuracy: 0.9231\n",
            "{'loss': 0.26706254482269287, 'sparse_categorical_accuracy': 0.9220200181007385}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 0.0402759350836277\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 0.008519379422068596\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.0037638770882040262\n",
            "Epoch 7/10\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = False\n",
            "Layer dense2 trainable = False\n",
            "Layer predictions trainable = True\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2433 - sparse_categorical_accuracy: 0.9275\n",
            "{'loss': 0.25018778443336487, 'sparse_categorical_accuracy': 0.925819993019104}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 0.033393971621990204\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 0.007702467497438192\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.0032134242355823517\n",
            "Epoch 8/10\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = False\n",
            "Layer dense2 trainable = False\n",
            "Layer predictions trainable = True\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2354 - sparse_categorical_accuracy: 0.9321\n",
            "{'loss': 0.23663820326328278, 'sparse_categorical_accuracy': 0.9297599792480469}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 0.029509885236620903\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 0.007362808100879192\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.0029434477910399437\n",
            "Epoch 9/10\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = False\n",
            "Layer dense2 trainable = False\n",
            "Layer predictions trainable = True\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2217 - sparse_categorical_accuracy: 0.9340\n",
            "{'loss': 0.22428421676158905, 'sparse_categorical_accuracy': 0.9327399730682373}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 0.026639552786946297\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 0.0058114295825362206\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.002770393854007125\n",
            "Epoch 10/10\n",
            "Trainable status:\n",
            "Layer inputs trainable = True\n",
            "Layer dense1 trainable = False\n",
            "Layer dense2 trainable = False\n",
            "Layer predictions trainable = True\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.2173 - sparse_categorical_accuracy: 0.9349\n",
            "{'loss': 0.21340078115463257, 'sparse_categorical_accuracy': 0.9359800219535828}\n",
            "Layer dense1 changed? [('dense1/kernel:0', True), ('dense1/bias:0', True)]\n",
            "sqr diff 0.023647833615541458\n",
            "Layer dense2 changed? [('dense2/kernel:0', True), ('dense2/bias:0', True)]\n",
            "sqr diff 0.004862810485064983\n",
            "Layer predictions changed? [('predictions/kernel:0', True), ('predictions/bias:0', True)]\n",
            "sqr diff 0.002078779274597764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe36768d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}